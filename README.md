# iisreader
Iisreader analyzes the log generated by the Internet information server.
It summarizes the number of requests, response times and average responses times for different types of requests and inserts the results into an excel spreadsheet
It can extract a set of defined requests and give a report for these requests
It can be customized for scheduled tasks so that it automatically can provide, e.g. a daily or weekly, report.

## flags

iisreader is a command based tool with several parameters to customize your report. It comes with the following flags:

### level of detail: page | ip | status | all: 
-detail default: page, which reports the ".aspx" requests, "page", "level of detail: page | ip | status | all")
### filter:  "list of strings"
-filter default: ".aspx"

### verbose: writes the output to the screen
-v default: true
### reportname: name of the excel report
-reportname default: "Logreport.xlsx"
### mail: send mail via localhost, port 25 
-m default: false     

### hostname: host mail server
-hostname default: localhost

### port: mail port
-p default: 25

### to: receiver of mail
-to default: 

### from: sender
-from default: logreport@kimik-it.gl

### days: number of days that reports should be generated for starting from the previous day
-days default: 0 which means that the program uses the filenames specified as parameter on cmd line

## Configuration Examples

The standard usage for an ad hoc report is *iislogreader $<$filename$>$*, e.g. *iislogreader u_ex210309.log*
This command will generate an excel file with the default name **Logreport.xlsx** containing the requests containing the string ".aspx" in the url of the request
and write the content of the report to the screen.

If you want to filter the requests differently you should use the parameter -filter and provide the strings that the program should search for in the URL.
Thus *-filter ".aspx api"* would give not only aspx pages but also web api requests located in the api subdirectory (in Microsoft .Net environments)

If you only want to see a report for a single or a few pages you would set the filter thus: *-filter "mypage.aspx mypage1.html"*

If you're interested in HTTP return codes you can specify *-detail status* whih will provide a report containing statuscodes and number of occurences

Similary you can specify *-detail ip* if you want a report based on IP addresses containing users and numbers of requests from that IP address

The standard usage for a scheduled job for a daily report is  *iislogreader -days 1 -m=true -to abc@gmail.com -v=false*
This will generate an excel file and send it to abc@gmail.com on a daily basis, provided that the scheduled task is set to run once a day

A weekly report is generated by increaing the number in the days parameter to 7 and running the task once a week, e.g. *iislogreader -days 7 -m=true -to abc@gmail.com* 

